{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "\n",
    "# Deep learning libraries\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import PIL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image generators for importing data and providing validation split for training\n",
    "train_image = ImageDataGenerator(rescale=1/255,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 shear_range=10,\n",
    "                                 brightness_range=[0.95,1.05],\n",
    "                                 validation_split=.2)\n",
    "\n",
    "#attempting to create validation data set without augmentations applied to the training set\n",
    "#random seed is set upon generation of the data, thus should prevent data leakage\n",
    "validation_image = ImageDataGenerator(rescale=1/255,\n",
    "                                     validation_split=.2)\n",
    "\n",
    "test_image = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4186 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data processing\n",
    "train_gen = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1045 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#validation data processing\n",
    "val_gen = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#testing data processing\n",
    "test_gen = test_image.flow_from_directory(\n",
    "    directory='chest_xray/test', \n",
    "    target_size=(128, 128), color_mode='grayscale', \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.8505 - accuracy: 0.6149"
     ]
    }
   ],
   "source": [
    "#initial model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2nd model iteration - increased kernel size, added padding\n",
    "kernel = (8, 8)\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(128, activation='relu'))\n",
    "model2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model2.fit(train_gen,epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3rd model iteration - increased pool size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model3.add(layers.MaxPooling2D(pool_size=pool))\n",
    "\n",
    "#hidden layers\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.MaxPooling2D(pool))\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(128, activation='relu'))\n",
    "model3.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model3.fit(train_gen,epochs=10,validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4th model iteration - changed activation function for hidden layers to tanh, converted last hidden layer to dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model4 = models.Sequential()\n",
    "model4.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model4.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.MaxPooling2D(pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(128, activation='tanh'))\n",
    "model4.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model4.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model4.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5th model iteration - reverted activation function changes, kept dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model5 = models.Sequential()\n",
    "model5.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model5.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.MaxPooling2D(pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(128, activation='relu'))\n",
    "model5.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model5.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5th model iteration - added 1 extra convlution, reduced kernel size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model6 = models.Sequential()\n",
    "model6.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model6.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.MaxPooling2D(pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(128, activation='relu'))\n",
    "model6.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model6.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model6.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model6.fit(train_gen,epochs=20,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Image Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the granularity typically involved in human interpretation of Chest X-ray images (i.e. identifying presence and patterns of infiltrate in the lungs) we are experimenting with increasing the size of the images to achieve greater precision. This poses a greater risk of overfitting, and increases training time, however, it is possible that using convolutions and pooling, our model may be able to aggregate and interpret this finer detail if tuned appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data processing using half length and width of original image size\n",
    "train_gen_0 = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=64, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)\n",
    "\n",
    "#validation data processing using half length and width of original image size\n",
    "val_gen_0 = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=128, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial model using half length and width of original image size\n",
    "model7 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model7.add(layers.Conv2D(filters=3,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='swish',\n",
    "                        input_shape=(534, 381, 1)))\n",
    "model7.add(layers.Flatten())\n",
    "\n",
    "#hidden layers\n",
    "model7.add(layers.Dense(128, activation='swish'))\n",
    "model7.add(layers.Dense(64, activation='swish'))\n",
    "model7.add(layers.Dense(32, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model7.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model7.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model7.fit(train_gen_0, epochs=20, validation_data=val_gen_0, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going Deeper\n",
    "model8 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model8.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model8.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.Flatten())\n",
    "model8.add(layers.Dense(64, activation='relu'))\n",
    "model8.add(layers.Dense(32, activation='relu'))\n",
    "model8.add(layers.Dense(16, activation='relu'))\n",
    "model8.add(layers.Dense(8, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model8.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model8.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model8.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going Even Deeper\n",
    "model9 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model9.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model9.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model9.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model9.add(layers.Flatten())\n",
    "#dense layers for interpretation\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model9.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model9.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model9.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Went too deep - trying new activation function in hidden layers\n",
    "model0 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model0.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model0.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.MaxPooling2D((2, 2)))\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.Flatten())\n",
    "model0.add(layers.Dense(64, activation='swish'))\n",
    "model0.add(layers.Dense(32, activation='swish'))\n",
    "model0.add(layers.Dense(16, activation='swish'))\n",
    "model0.add(layers.Dense(8, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model0.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model0.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model0.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
