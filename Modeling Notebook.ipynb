{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that much of the code for different models has been pushed out into markdown cells after having been run. This is due to severe memory bloat issues during training that caused errors to arise during training. An attempt was made to find other workarounds, but so far they have been unsuccessful***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "\n",
    "# Deep learning libraries\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import PIL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image generators for importing data and providing validation split for training\n",
    "train_image = ImageDataGenerator(rescale=1/255,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 shear_range=10,\n",
    "                                 brightness_range=[0.95,1.05],\n",
    "                                 validation_split=.2)\n",
    "\n",
    "#attempting to create validation data set without augmentations applied to the training set\n",
    "#random seed is set upon generation of the data, thus should prevent data leakage\n",
    "validation_image = ImageDataGenerator(rescale=1/255,\n",
    "                                     validation_split=.2)\n",
    "\n",
    "test_image = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training data processing\n",
    "train_gen = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#validation data processing\n",
    "val_gen = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#testing data processing\n",
    "test_gen = test_image.flow_from_directory(\n",
    "    directory='chest_xray/test', \n",
    "    target_size=(128, 128), color_mode='grayscale', \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#initial model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#2nd model iteration - increased kernel size, added padding\n",
    "kernel = (8, 8)\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(128, activation='relu'))\n",
    "model2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model2.fit(train_gen,epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#3rd model iteration - increased pool size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model3.add(layers.MaxPooling2D(pool_size=pool))\n",
    "\n",
    "#hidden layers\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.MaxPooling2D(pool))\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(128, activation='relu'))\n",
    "model3.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model3.fit(train_gen,epochs=10,validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4th model iteration - changed activation function for hidden layers to tanh, converted last hidden layer to dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model4 = models.Sequential()\n",
    "model4.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model4.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.MaxPooling2D(pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(128, activation='tanh'))\n",
    "model4.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model4.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model4.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5th model iteration - reverted activation function changes, kept dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model5 = models.Sequential()\n",
    "model5.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model5.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.MaxPooling2D(pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(128, activation='relu'))\n",
    "model5.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model5.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5th model iteration - added 1 extra convlution, reduced kernel size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model6 = models.Sequential()\n",
    "model6.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model6.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.MaxPooling2D(pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(128, activation='relu'))\n",
    "model6.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model6.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model6.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model6.fit(train_gen,epochs=20,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Image Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the granularity typically involved in human interpretation of Chest X-ray images (i.e. identifying presence and patterns of infiltrate in the lungs) we are experimenting with increasing the size of the images to achieve greater precision. This poses a greater risk of overfitting, and increases training time, however, it is possible that using convolutions and pooling, our model may be able to aggregate and interpret this finer detail if tuned appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training data processing using half length and width of original image size\n",
    "train_gen_0 = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=64, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)\n",
    "\n",
    "#validation data processing using half length and width of original image size\n",
    "val_gen_0 = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=128, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#initial model using half length and width of original image size\n",
    "model7 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model7.add(layers.Conv2D(filters=3,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='swish',\n",
    "                        input_shape=(534, 381, 1)))\n",
    "model7.add(layers.Flatten())\n",
    "\n",
    "#hidden layers\n",
    "model7.add(layers.Dense(128, activation='swish'))\n",
    "model7.add(layers.Dense(64, activation='swish'))\n",
    "model7.add(layers.Dense(32, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model7.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model7.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model7.fit(train_gen_0, epochs=20, validation_data=val_gen_0, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Going Deeper\n",
    "model8 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model8.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model8.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.Flatten())\n",
    "model8.add(layers.Dense(64, activation='relu'))\n",
    "model8.add(layers.Dense(32, activation='relu'))\n",
    "model8.add(layers.Dense(16, activation='relu'))\n",
    "model8.add(layers.Dense(8, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model8.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model8.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model8.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Going Even Deeper\n",
    "model9 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model9.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model9.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model9.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model9.add(layers.Flatten())\n",
    "#dense layers for interpretation\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model9.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model9.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model9.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Went too deep - trying new activation function in hidden layers\n",
    "model0 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model0.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model0.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.MaxPooling2D((2, 2)))\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.Flatten())\n",
    "model0.add(layers.Dense(64, activation='swish'))\n",
    "model0.add(layers.Dense(32, activation='swish'))\n",
    "model0.add(layers.Dense(16, activation='swish'))\n",
    "model0.add(layers.Dense(8, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model0.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model0.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model0.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the final model, data from the validation split should be re-incorporated for training and the final evaluation will be performed on the formal test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image generators for importing data - no validation split\n",
    "train_image = ImageDataGenerator(rescale=1/255,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 shear_range=10,\n",
    "                                 brightness_range=[0.95,1.05])\n",
    "\n",
    "test_image = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5231 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data processing\n",
    "train_gen = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "164/164 [==============================] - 163s 994ms/step - loss: 0.8166 - accuracy: 0.6186 - auc: 0.8033 - val_loss: 0.6569 - val_accuracy: 0.7484 - val_auc: 0.8905\n",
      "Epoch 2/50\n",
      "164/164 [==============================] - 163s 992ms/step - loss: 0.6749 - accuracy: 0.7029 - auc: 0.8703 - val_loss: 0.7087 - val_accuracy: 0.7324 - val_auc: 0.8747\n",
      "Epoch 3/50\n",
      "164/164 [==============================] - 165s 1s/step - loss: 0.6113 - accuracy: 0.7379 - auc: 0.8922 - val_loss: 0.7672 - val_accuracy: 0.7308 - val_auc: 0.8659\n",
      "Epoch 4/50\n",
      "164/164 [==============================] - 159s 967ms/step - loss: 0.5919 - accuracy: 0.7360 - auc: 0.8984 - val_loss: 0.7252 - val_accuracy: 0.7644 - val_auc: 0.8850\n",
      "Epoch 5/50\n",
      "164/164 [==============================] - 161s 981ms/step - loss: 0.5631 - accuracy: 0.7563 - auc: 0.9086 - val_loss: 0.5734 - val_accuracy: 0.8061 - val_auc: 0.9185\n",
      "Epoch 6/50\n",
      "164/164 [==============================] - 160s 974ms/step - loss: 0.5436 - accuracy: 0.7616 - auc: 0.9143 - val_loss: 0.6433 - val_accuracy: 0.7837 - val_auc: 0.9069\n",
      "Epoch 7/50\n",
      "164/164 [==============================] - 159s 968ms/step - loss: 0.5323 - accuracy: 0.7635 - auc: 0.9177 - val_loss: 0.7573 - val_accuracy: 0.7532 - val_auc: 0.8857\n",
      "Epoch 8/50\n",
      "164/164 [==============================] - 159s 971ms/step - loss: 0.5347 - accuracy: 0.7695 - auc: 0.9170 - val_loss: 0.6932 - val_accuracy: 0.7500 - val_auc: 0.8881\n",
      "Epoch 9/50\n",
      "164/164 [==============================] - 160s 978ms/step - loss: 0.5148 - accuracy: 0.7754 - auc: 0.9217 - val_loss: 0.6722 - val_accuracy: 0.7804 - val_auc: 0.8967\n",
      "Epoch 10/50\n",
      "164/164 [==============================] - 160s 976ms/step - loss: 0.5021 - accuracy: 0.7777 - auc: 0.9267 - val_loss: 0.6232 - val_accuracy: 0.7756 - val_auc: 0.9065\n",
      "Epoch 11/50\n",
      "164/164 [==============================] - 159s 970ms/step - loss: 0.4986 - accuracy: 0.7809 - auc: 0.9272 - val_loss: 0.5391 - val_accuracy: 0.8253 - val_auc: 0.9291\n",
      "Epoch 12/50\n",
      "164/164 [==============================] - 164s 1s/step - loss: 0.4828 - accuracy: 0.7884 - auc: 0.9312 - val_loss: 0.7370 - val_accuracy: 0.7772 - val_auc: 0.8934\n",
      "Epoch 13/50\n",
      "164/164 [==============================] - 162s 987ms/step - loss: 0.5006 - accuracy: 0.7761 - auc: 0.9264 - val_loss: 0.6246 - val_accuracy: 0.7660 - val_auc: 0.9060\n",
      "Epoch 14/50\n",
      "164/164 [==============================] - 171s 1s/step - loss: 0.4908 - accuracy: 0.7846 - auc: 0.9294 - val_loss: 0.7499 - val_accuracy: 0.7660 - val_auc: 0.8905\n",
      "Epoch 15/50\n",
      "164/164 [==============================] - 167s 1s/step - loss: 0.4809 - accuracy: 0.7897 - auc: 0.9326 - val_loss: 0.6890 - val_accuracy: 0.7756 - val_auc: 0.9081\n",
      "Epoch 16/50\n",
      "164/164 [==============================] - 172s 1s/step - loss: 0.4816 - accuracy: 0.7876 - auc: 0.9319 - val_loss: 0.7401 - val_accuracy: 0.7500 - val_auc: 0.8892\n",
      "Epoch 17/50\n",
      "164/164 [==============================] - 168s 1s/step - loss: 0.4700 - accuracy: 0.7886 - auc: 0.9341 - val_loss: 0.7465 - val_accuracy: 0.7901 - val_auc: 0.8940\n",
      "Epoch 18/50\n",
      "164/164 [==============================] - 163s 993ms/step - loss: 0.4800 - accuracy: 0.7859 - auc: 0.9321 - val_loss: 0.7021 - val_accuracy: 0.7692 - val_auc: 0.8962\n",
      "Epoch 19/50\n",
      "164/164 [==============================] - 163s 993ms/step - loss: 0.4711 - accuracy: 0.7920 - auc: 0.9347 - val_loss: 0.6553 - val_accuracy: 0.7612 - val_auc: 0.9032\n",
      "Epoch 20/50\n",
      "164/164 [==============================] - 160s 977ms/step - loss: 0.4716 - accuracy: 0.7928 - auc: 0.9338 - val_loss: 0.5368 - val_accuracy: 0.8221 - val_auc: 0.9308\n",
      "Epoch 21/50\n",
      "164/164 [==============================] - 160s 977ms/step - loss: 0.4628 - accuracy: 0.7964 - auc: 0.9366 - val_loss: 0.5824 - val_accuracy: 0.7933 - val_auc: 0.9221\n",
      "Epoch 22/50\n",
      "164/164 [==============================] - 162s 989ms/step - loss: 0.4547 - accuracy: 0.8006 - auc: 0.9387 - val_loss: 0.5194 - val_accuracy: 0.8125 - val_auc: 0.9345\n",
      "Epoch 23/50\n",
      "164/164 [==============================] - 163s 994ms/step - loss: 0.4528 - accuracy: 0.8002 - auc: 0.9393 - val_loss: 0.6389 - val_accuracy: 0.7853 - val_auc: 0.9193\n",
      "Epoch 24/50\n",
      "164/164 [==============================] - 160s 975ms/step - loss: 0.4608 - accuracy: 0.7985 - auc: 0.9371 - val_loss: 0.7203 - val_accuracy: 0.7756 - val_auc: 0.8968\n",
      "Epoch 25/50\n",
      "164/164 [==============================] - 162s 987ms/step - loss: 0.4446 - accuracy: 0.8033 - auc: 0.9417 - val_loss: 0.7124 - val_accuracy: 0.7580 - val_auc: 0.8867\n",
      "Epoch 26/50\n",
      "164/164 [==============================] - 161s 984ms/step - loss: 0.4542 - accuracy: 0.8025 - auc: 0.9396 - val_loss: 0.7011 - val_accuracy: 0.7821 - val_auc: 0.9018\n",
      "Epoch 27/50\n",
      "164/164 [==============================] - 160s 973ms/step - loss: 0.4470 - accuracy: 0.8025 - auc: 0.9404 - val_loss: 0.5845 - val_accuracy: 0.8237 - val_auc: 0.9258\n",
      "Epoch 28/50\n",
      "164/164 [==============================] - 162s 989ms/step - loss: 0.4318 - accuracy: 0.8123 - auc: 0.9444 - val_loss: 0.7728 - val_accuracy: 0.7612 - val_auc: 0.8884\n",
      "Epoch 29/50\n",
      "164/164 [==============================] - 164s 1s/step - loss: 0.4411 - accuracy: 0.8079 - auc: 0.9424 - val_loss: 0.7546 - val_accuracy: 0.7676 - val_auc: 0.8920\n",
      "Epoch 30/50\n",
      "164/164 [==============================] - 161s 983ms/step - loss: 0.4299 - accuracy: 0.8144 - auc: 0.9446 - val_loss: 0.6063 - val_accuracy: 0.8109 - val_auc: 0.9195\n",
      "Epoch 31/50\n",
      "164/164 [==============================] - 161s 983ms/step - loss: 0.4352 - accuracy: 0.8115 - auc: 0.9439 - val_loss: 0.6498 - val_accuracy: 0.7981 - val_auc: 0.9117\n",
      "Epoch 32/50\n",
      "164/164 [==============================] - 160s 977ms/step - loss: 0.4195 - accuracy: 0.8165 - auc: 0.9476 - val_loss: 0.6863 - val_accuracy: 0.7837 - val_auc: 0.9061\n",
      "Epoch 33/50\n",
      "164/164 [==============================] - 160s 974ms/step - loss: 0.4295 - accuracy: 0.8163 - auc: 0.9454 - val_loss: 0.7703 - val_accuracy: 0.7644 - val_auc: 0.8936\n",
      "Epoch 34/50\n",
      "164/164 [==============================] - 160s 976ms/step - loss: 0.4254 - accuracy: 0.8119 - auc: 0.9463 - val_loss: 0.6046 - val_accuracy: 0.7804 - val_auc: 0.9162\n",
      "Epoch 35/50\n",
      "164/164 [==============================] - 160s 979ms/step - loss: 0.4203 - accuracy: 0.8153 - auc: 0.9475 - val_loss: 0.7624 - val_accuracy: 0.7660 - val_auc: 0.8958\n",
      "Epoch 36/50\n",
      "164/164 [==============================] - 162s 990ms/step - loss: 0.4443 - accuracy: 0.8058 - auc: 0.9421 - val_loss: 0.5631 - val_accuracy: 0.8077 - val_auc: 0.9265\n",
      "Epoch 37/50\n",
      "164/164 [==============================] - 160s 974ms/step - loss: 0.4274 - accuracy: 0.8100 - auc: 0.9457 - val_loss: 0.6007 - val_accuracy: 0.7981 - val_auc: 0.9174\n",
      "Epoch 38/50\n",
      "164/164 [==============================] - 160s 973ms/step - loss: 0.4215 - accuracy: 0.8184 - auc: 0.9476 - val_loss: 0.5425 - val_accuracy: 0.8237 - val_auc: 0.9323\n",
      "Epoch 39/50\n",
      "164/164 [==============================] - 159s 971ms/step - loss: 0.4273 - accuracy: 0.8186 - auc: 0.9462 - val_loss: 0.6036 - val_accuracy: 0.8189 - val_auc: 0.9263\n",
      "Epoch 40/50\n",
      "164/164 [==============================] - 159s 969ms/step - loss: 0.4204 - accuracy: 0.8218 - auc: 0.9477 - val_loss: 0.6237 - val_accuracy: 0.7917 - val_auc: 0.9163\n",
      "Epoch 41/50\n",
      "164/164 [==============================] - 160s 978ms/step - loss: 0.4212 - accuracy: 0.8117 - auc: 0.9474 - val_loss: 0.6764 - val_accuracy: 0.7708 - val_auc: 0.9093\n",
      "Epoch 42/50\n",
      "164/164 [==============================] - 164s 1s/step - loss: 0.4139 - accuracy: 0.8239 - auc: 0.9492 - val_loss: 0.7781 - val_accuracy: 0.7724 - val_auc: 0.8946\n",
      "Epoch 43/50\n",
      "164/164 [==============================] - 162s 986ms/step - loss: 0.4191 - accuracy: 0.8153 - auc: 0.9480 - val_loss: 0.8152 - val_accuracy: 0.7356 - val_auc: 0.8787\n",
      "Epoch 44/50\n",
      "164/164 [==============================] - 164s 1s/step - loss: 0.4110 - accuracy: 0.8232 - auc: 0.9497 - val_loss: 0.5852 - val_accuracy: 0.8205 - val_auc: 0.9301\n",
      "Epoch 45/50\n",
      "164/164 [==============================] - 163s 995ms/step - loss: 0.4100 - accuracy: 0.8260 - auc: 0.9499 - val_loss: 0.6900 - val_accuracy: 0.7885 - val_auc: 0.9086\n",
      "Epoch 46/50\n",
      "164/164 [==============================] - 163s 997ms/step - loss: 0.3931 - accuracy: 0.8329 - auc: 0.9545 - val_loss: 0.7011 - val_accuracy: 0.7917 - val_auc: 0.9126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "164/164 [==============================] - 162s 986ms/step - loss: 0.4001 - accuracy: 0.8268 - auc: 0.9526 - val_loss: 0.6761 - val_accuracy: 0.8013 - val_auc: 0.9178\n",
      "Epoch 48/50\n",
      "164/164 [==============================] - 162s 988ms/step - loss: 0.4067 - accuracy: 0.8222 - auc: 0.9512 - val_loss: 0.8169 - val_accuracy: 0.7788 - val_auc: 0.8932\n",
      "Epoch 49/50\n",
      "164/164 [==============================] - 163s 992ms/step - loss: 0.3935 - accuracy: 0.8264 - auc: 0.9539 - val_loss: 0.6765 - val_accuracy: 0.7837 - val_auc: 0.9125\n",
      "Epoch 50/50\n",
      "164/164 [==============================] - 162s 988ms/step - loss: 0.4000 - accuracy: 0.8266 - auc: 0.9525 - val_loss: 0.7742 - val_accuracy: 0.7660 - val_auc: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a618f5c40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Model\n",
    "Final = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "Final.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "Final.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "Final.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "Final.add(layers.MaxPooling2D((2, 2)))\n",
    "Final.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "Final.add(layers.Flatten())\n",
    "Final.add(layers.Dense(64, activation='swish'))\n",
    "Final.add(layers.Dense(32, activation='swish'))\n",
    "Final.add(layers.Dense(16, activation='swish'))\n",
    "Final.add(layers.Dense(8, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "Final.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "Final.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "#validation data param is used here to preview performance but not used for training\n",
    "Final.fit(train_gen, epochs=50, validation_data=test_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
