{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that much of the code for different models has been pushed out into markdown cells after having been run. This is due to severe memory bloat issues during training that caused errors to arise during training. An attempt was made to find other workarounds, but so far they have been unsuccessful***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "\n",
    "# Deep learning libraries\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import PIL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image generators for importing data and providing validation split for training\n",
    "train_image = ImageDataGenerator(rescale=1/255,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 shear_range=10,\n",
    "                                 brightness_range=[0.95,1.05],\n",
    "                                 validation_split=.2)\n",
    "\n",
    "#attempting to create validation data set without augmentations applied to the training set\n",
    "#random seed is set upon generation of the data, thus should prevent data leakage\n",
    "validation_image = ImageDataGenerator(rescale=1/255,\n",
    "                                     validation_split=.2)\n",
    "\n",
    "test_image = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training data processing\n",
    "train_gen = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#validation data processing\n",
    "val_gen = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#testing data processing\n",
    "test_gen = test_image.flow_from_directory(\n",
    "    directory='chest_xray/test', \n",
    "    target_size=(128, 128), color_mode='grayscale', \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#initial model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#2nd model iteration - increased kernel size, added padding\n",
    "kernel = (8, 8)\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(128, activation='relu'))\n",
    "model2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model2.fit(train_gen,epochs=10, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#3rd model iteration - increased pool size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "model3.add(layers.MaxPooling2D(pool_size=pool))\n",
    "\n",
    "#hidden layers\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.MaxPooling2D(pool))\n",
    "model3.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(128, activation='relu'))\n",
    "model3.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model3.fit(train_gen,epochs=10,validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4th model iteration - changed activation function for hidden layers to tanh, converted last hidden layer to dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model4 = models.Sequential()\n",
    "model4.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model4.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.MaxPooling2D(pool))\n",
    "model4.add(layers.Conv2D(128, kernel, activation='tanh'))\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(128, activation='tanh'))\n",
    "model4.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model4.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model4.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5th model iteration - reverted activation function changes, kept dropout layer\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model5 = models.Sequential()\n",
    "model5.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model5.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.MaxPooling2D(pool))\n",
    "model5.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(128, activation='relu'))\n",
    "model5.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model5.fit(train_gen,epochs=10,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5th model iteration - added 1 extra convlution, reduced kernel size\n",
    "kernel = (8, 8)\n",
    "pool = (3, 3)\n",
    "\n",
    "model6 = models.Sequential()\n",
    "model6.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=kernel,\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1),\n",
    "                        padding='valid'))\n",
    "#hidden layers\n",
    "model6.add(layers.MaxPooling2D(pool_size=pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.MaxPooling2D(pool))\n",
    "model6.add(layers.Conv2D(128, kernel, activation='relu'))\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(128, activation='relu'))\n",
    "model6.add(layers.Dropout(rate=.25))\n",
    "#output layer\n",
    "model6.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compiling and fitting the model\n",
    "model6.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model6.fit(train_gen,epochs=20,validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Image Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the granularity typically involved in human interpretation of Chest X-ray images (i.e. identifying presence and patterns of infiltrate in the lungs) we are experimenting with increasing the size of the images to achieve greater precision. This poses a greater risk of overfitting, and increases training time, however, it is possible that using convolutions and pooling, our model may be able to aggregate and interpret this finer detail if tuned appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training data processing using half length and width of original image size\n",
    "train_gen_0 = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=64, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)\n",
    "\n",
    "#validation data processing using half length and width of original image size\n",
    "val_gen_0 = validation_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(534, 381),color_mode='grayscale',\n",
    "    batch_size=128, \n",
    "    class_mode='categorical', subset='validation', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#initial model using half length and width of original image size\n",
    "model7 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model7.add(layers.Conv2D(filters=3,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='swish',\n",
    "                        input_shape=(534, 381, 1)))\n",
    "model7.add(layers.Flatten())\n",
    "\n",
    "#hidden layers\n",
    "model7.add(layers.Dense(128, activation='swish'))\n",
    "model7.add(layers.Dense(64, activation='swish'))\n",
    "model7.add(layers.Dense(32, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model7.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model7.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model7.fit(train_gen_0, epochs=20, validation_data=val_gen_0, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Going Deeper\n",
    "model8 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model8.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model8.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "model8.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model8.add(layers.Flatten())\n",
    "model8.add(layers.Dense(64, activation='relu'))\n",
    "model8.add(layers.Dense(32, activation='relu'))\n",
    "model8.add(layers.Dense(16, activation='relu'))\n",
    "model8.add(layers.Dense(8, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model8.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model8.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model8.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Going Even Deeper\n",
    "model9 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model9.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model9.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model9.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model9.add(layers.Flatten())\n",
    "#dense layers for interpretation\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.5))\n",
    "model9.add(layers.Dense(64, activation=layers.LeakyReLU()))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "model9.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model9.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model9.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Went too deep - trying new activation function in hidden layers\n",
    "model0 = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "model0.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "model0.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.MaxPooling2D((2, 2)))\n",
    "model0.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "model0.add(layers.Flatten())\n",
    "model0.add(layers.Dense(64, activation='swish'))\n",
    "model0.add(layers.Dense(32, activation='swish'))\n",
    "model0.add(layers.Dense(16, activation='swish'))\n",
    "model0.add(layers.Dense(8, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "model0.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "model0.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "model0.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the final model, data from the validation split should be re-incorporated for training and the final evaluation will be performed on the formal test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image generators for importing data - no validation split\n",
    "train_image = ImageDataGenerator(rescale=1/255,\n",
    "                                 width_shift_range=0.05,\n",
    "                                 height_shift_range=0.05,\n",
    "                                 horizontal_flip=True,\n",
    "                                 shear_range=10,\n",
    "                                 brightness_range=[0.95,1.05])\n",
    "\n",
    "test_image = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5231 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data processing\n",
    "train_gen = train_image.flow_from_directory(\n",
    "    directory='chest_xray/train', \n",
    "    target_size=(128, 128),color_mode='grayscale',\n",
    "    batch_size=32, \n",
    "    class_mode='categorical', subset='training', interpolation=\"lanczos\",\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 60/164 [=========>....................] - ETA: 1:31 - loss: 1.0460 - accuracy: 0.4887 - auc: 0.6443"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b342f76d9ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                            save_best_only=True)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mFinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "Final = models.Sequential()\n",
    "\n",
    "#input layer\n",
    "Final.add(layers.Conv2D(filters=16,\n",
    "                        kernel_size=(20, 20),\n",
    "                        activation='relu',\n",
    "                        input_shape=(128, 128, 1)))\n",
    "Final.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden layers\n",
    "Final.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "Final.add(layers.MaxPooling2D((2, 2)))\n",
    "Final.add(layers.Conv2D(32, (2, 2), activation='swish'))\n",
    "Final.add(layers.Flatten())\n",
    "Final.add(layers.Dense(64, activation='swish'))\n",
    "Final.add(layers.Dense(32, activation='swish'))\n",
    "Final.add(layers.Dense(16, activation='swish'))\n",
    "Final.add(layers.Dense(8, activation='swish'))\n",
    "\n",
    "#output layer\n",
    "Final.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "#compile and compute\n",
    "Final.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'AUC'])\n",
    "#creating a save file for this model\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./Checkpoints\",\n",
    "                                           save_weights_only=True,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           mode='max',\n",
    "                                           save_best_only=True)\n",
    "\n",
    "Final.fit(train_gen, epochs=50, callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
